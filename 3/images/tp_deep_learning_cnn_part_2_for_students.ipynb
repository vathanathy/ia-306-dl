{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KgrfZuOKA6r"
   },
   "source": [
    "# TP Coding Convolutional Neural Networks in tensorflow and keras - part 2\n",
    "\n",
    "Author : Alasdair Newson\n",
    "\n",
    "alasdair.newson@telecom-paris.fr\n",
    "\n",
    "In this session, we shall be looking at two subjects :\n",
    "\n",
    "- A way to visualise what networks are learning : the Deep Dream algorithm\n",
    "- Adversarial examples\n",
    "\n",
    "At the heart of these two applications is the calculation of the gradient of a loss function with respect to the image itself (instead of respect to the weights). The loss function will be defined depending on the application at hand.\n",
    "\n",
    "We shall use a pre-trained, well-known pretrained network. The lab has been designed for either VGG16 or inceptionv3. You can try with other networks, but bear in mind that the pre-processing operations are different for each network, so you would have to change the code correspondingly.\n",
    "\n",
    "We can easily access certain well-known networks with the Tensorflow programming framework. There are useful predefined functions which allow us to load the weights, view the architecture etc. of the networks. We will specify these functions as necessary through the lab work. Unfortunately, the documentation for these functions is not very plentiful, but if you want to look at exactly what they do, you can look at the source code for help :\n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_v3.py\n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py\n",
    "\n",
    "However, we will indicate how these functions work as the need arises.\n",
    "\n",
    "\n",
    "First, let's load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtjQ-Fl4KA6s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.layers import Input\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49mhdiz8KA6x"
   },
   "source": [
    "Now, let's load one of the most famous networks, VGG16 or inceptionv3 (you choose), and view it's architecture with the summary() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mlxxtnk_KA6x"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'inceptionv3'#'vgg16'#\n",
    "\n",
    "if model_name == 'inceptionv3':\n",
    "\tmodel = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "elif model_name == 'vgg16':\n",
    "\tmodel = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqNPEKffKA7A"
   },
   "source": [
    "You can also extract the weights of the convolutional filters in the deep CNN. You can do this with the following function :\n",
    "\n",
    "- weights = layer.get_weights()\n",
    "\n",
    "where 'layer' iterates throughout the 'model.layers' list. Note that this function returns the following :\n",
    "- [weights, biases] if the layer is convolutional. weights has size [m,n,filter_depth,n_filters]\n",
    "- an empty list otherwise\n",
    "\n",
    "Write a function which retrieves the weights of the network (you will have to test if the weights variable is empty at each iteration).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0qB0kTPKA7B"
   },
   "outputs": [],
   "source": [
    "def retrieve_weights(model):\n",
    "    weight_list = []\n",
    "    # BEGIN CODE HERE\n",
    "    # END CODE HERE\n",
    "    return weight_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEC3QZbuKA7F"
   },
   "source": [
    "Now, write a code that visualises a single channel of a filter of your choice. Reminder, to view a grey-level image, you can use :\n",
    "\n",
    "- plt.imshow(img,cmap=\"gray\")\n",
    "- plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqwJRrWUKA7F"
   },
   "outputs": [],
   "source": [
    "weight_list = retrieve_weights(model)\n",
    "curr_weight = weight_list[2]\n",
    "curr_weight.shape\n",
    "plt.imshow(curr_weight[:,:,0,1],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_GP-VFVKA7L"
   },
   "outputs": [],
   "source": [
    "curr_weight[:,:,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMXb4PeXKA7P"
   },
   "source": [
    "As you can probably see, this visualisation is not of much use : we cannot really tell what is going on in the network. For this, let's turn to another approach : Deep Dream !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue4N3je6KA7Q"
   },
   "source": [
    "## 2. Deep Dream\n",
    "\n",
    "We now proceed to carry out the Deep Dream algorithm. The idea of the Deep Dream algorithm is to find an image which maximises the response of a network at a certain layer : $\\textbf{this should help us understand what the network is learning}$. This can be done with an iterative algorithm, by simply carrying out gradient $\\textbf{ascension}$. We start with an input image and iteratively add the gradient of the average response of the features which interest us. A pseudo-code for this would be :\n",
    "\n",
    "- img = img_in\n",
    "- for i=1:n_iters\n",
    "    - img = img + grad_step $\\nabla_{img} \\mathcal{L}$,\n",
    "    \n",
    "where $\\mathcal{L}$ is a function of the responses which interest us (you need to define this). In this part of the lab, we shall use the average response.\n",
    "\n",
    "Let's first define a function to preprocess the image. This is needed to put the image in the correct format for the VGG16 or inceptionv3 networks. We also create a function to invert this process. You can add cases for other networks if you wish (and you can get it working :) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib1WJOo5KA7R"
   },
   "outputs": [],
   "source": [
    "def format_image(img_file,model_name):\n",
    "\t\"\"\"\n",
    "\tThis function reads and formats an image so that it can be fed to the VGG16 or inceptionv3 networks\n",
    "\t\n",
    "\tParameters\n",
    "\t----------\n",
    "\timg_file : image file name\n",
    "\t\n",
    "\tReturns\n",
    "\t-------\n",
    "\timg_out_model : the correctly formatted image for VGG16 or inceptionv3 networks\n",
    "\timg : the image as read by the load_img function of keras.preprocessing.image\n",
    "\t\"\"\"\n",
    "\t# read image\n",
    "\timg = load_img(img_file)\n",
    "\t# convert image to an array\n",
    "\timg_out = img_to_array(img)\n",
    "\t# preprocess the image to put in the correct format for use with the VGG16/inceptionv3 network trained on imagenet\n",
    "\t\n",
    "\tif model_name == 'inceptionv3':\n",
    "\t\t#https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input\n",
    "\t\t#The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "\t\timg_out_model = tf.keras.applications.inception_v3.preprocess_input(img_out)\n",
    "\telif model_name == 'vgg16':\n",
    "\t\t#https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input\n",
    "\t\t#The images are converted from RGB to BGR,\n",
    "\t\t#then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "\t\timg_out_model = tf.keras.applications.vgg16.preprocess_input(img_out)\n",
    "\t# add a dimension at the beginning, coresponding to the batch dimension\n",
    "\timg_out_model = np.expand_dims(img_out_model, axis=0)\n",
    "\treturn img_out_model, img\n",
    "\n",
    "def unformat_image(img_in,model_name):\n",
    "\t\"\"\"\n",
    "\tThis function inverts the preprocessing applied to images for use in the VGG16/inceptionv3 network\n",
    "\t\n",
    "\tParameters\n",
    "\t----------\n",
    "\timg_file : formatted image of shape (batch_size,m,n,3)\n",
    "\t\n",
    "\tReturns\n",
    "\t-------\n",
    "\timg_out : a m-by-n-by-3 array, representing an image that can be written to an image file\n",
    "\t\"\"\"\n",
    "\t#get rid of batch dimension\n",
    "\timg_out=np.squeeze(img_in)\n",
    "\n",
    "\t\n",
    "\tif(model_name == 'inceptionv3'):\n",
    "\t\t#clamp image to the range [0,255] and cast to uint8\n",
    "\t\timg_out = 255*(img_out + 1.0)/2.0\n",
    "\tif (model_name == 'vgg16'):\n",
    "\t\t#https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input\n",
    "\t\t#The images are converted from RGB to BGR,\n",
    "\t\t#then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "\t\t#remove offsets added by the VGG16 preprocessing\n",
    "\t\timg_out[:, :, 0] += 103.939\n",
    "\t\timg_out[:, :, 1] += 116.779\n",
    "\t\timg_out[:, :, 2] += 123.68\n",
    "\t\t#invert the order of the colours : BGR -> RGB\n",
    "\t\timg_out = img_out[:, :, ::-1]\n",
    "\t\t#img_out = 255*(img_out + 1.0)/2.0\n",
    "\t\n",
    "\timg_out = np.clip(img_out, 0, 255).astype('uint8')\n",
    "\treturn img_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "570yJTBzKA7U"
   },
   "source": [
    "Now, we load the image. At the same time, we create a backend Tensor which has the correct format for the network.\n",
    "\n",
    "**NOTE** : if you are using colab, then you might not be able to easily upload the images of the lab work. In this case you can use download the image directly from the url given (this is done for you). To modify the behaviour, change the ```using_colab``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz37JIMqKA7V"
   },
   "outputs": [],
   "source": [
    "using_colab = True\n",
    "\n",
    "#load image\n",
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\"\n",
    "    img_in,img_visu = format_image('got.jpg',model_name)\n",
    "else:\n",
    "    img_in,img_visu = format_image('images/got.jpg',model_name)\n",
    "#show the input image\n",
    "plt.imshow(img_visu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykQiJhGyKA7Y"
   },
   "source": [
    "Now, we define the loss that we wish to maximise. This can be anything you wish, but a common loss is simply the average response of a certain channel of a certain layer. Since these responses are all positive, due to the non-linearities used, we can safely take the average as a loss function to maximise.\n",
    "\n",
    "We are going to use the responses of different layers depending on whether we use inceptionv3 or vgg16. You can modify the names of the layers to try and inspect different layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vn83PvRVKA7Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "if(model_name == 'inceptionv3'): \n",
    "\tlayer_names = ['mixed3', 'mixed5']\n",
    "elif (model_name == 'vgg16'):\n",
    "\tlayer_names = ['block5_conv2', 'block5_conv3']\n",
    "layers = [model.get_layer(name).output for name in layer_names]\n",
    "# create a model which outputs the layer which interests us\n",
    "img_backend = model.input\n",
    "deepdream_model = tf.keras.Model(inputs=img_backend, outputs=layers)\n",
    "print(deepdream_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_tnNojn-UU6"
   },
   "source": [
    "Now, we create the deep dream loss. \n",
    "\n",
    "The operations which calculate the loss should be done using the tensorflow functionalities, since we are working on symbolic Tensors. For example, use the following function for calculateing the mean :\n",
    "- tf.math.reduce_mean()\n",
    "and for calculating the total loss \n",
    "- tf.math.reduce_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52qz7Kh2-TUW"
   },
   "outputs": [],
   "source": [
    "def deep_dream_loss(model_in,img_in):\n",
    "\tactivation = model_in(img_in) \n",
    "\tif len(activation) == 1:\n",
    "\t\tactivation = [activation]\n",
    "\n",
    "\tlosses = []\n",
    "\tfor act in activation:\n",
    "\t\tloss = ... # FILL IN CODE\n",
    "\t\tlosses.append(loss)\n",
    "\ttotal_loss = ... # FILL IN CODE\n",
    "\n",
    "\treturn total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7noPIj6G-mwo"
   },
   "source": [
    "Now, we create a function to carry out one step of the gradient __ascent__. For this, we need to calculate the gradient of the loss with respect to the input image. \n",
    "\n",
    "This is done with the following syntax. For this example, we consider that ```x``` is the variable, and ```my_loss_function``` is the function which we want to take the derivative of.\n",
    "\n",
    "```\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    loss = my_loss_function(x)\n",
    "grads = tape.gradient(loss,x)\n",
    "```\n",
    "\n",
    "Next, we use a little trick. Indeed, it may be the case that the gradients are far too small or far too large for updating, meaning that the updates do nothing or destroy the image. To avoid this, we normalise the gradients, such that the standard deviation is 1. For this, use the following function :\n",
    "\n",
    "- tf.math.reduce_std()\n",
    "\n",
    "Finally, to make sure there are no division problems, we also add a small constant $\\epsilon $to the standard deviation. In other words :\n",
    "\n",
    "- $\\nabla = \\frac{\\nabla}{std(\\nabla) + \\epsilon}$\n",
    "\n",
    "Set this constant to 1e-8.\n",
    "\n",
    "Fill in the following function which carries out a step of the gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAeUI9EvmXMV"
   },
   "outputs": [],
   "source": [
    "def gradient_step(model_in, img_in, step_size, model_name):\n",
    "  # BEGIN STUDENT CODE\n",
    "\n",
    "  # END STUDENT CODE\n",
    "  img_in = ... # FILL IN CODE : CARRY OUT A STEP OF THE GRADIENT ASCENT\n",
    "\n",
    "  if (model_name == 'inceptionv3'):\n",
    "    img_in = tf.clip_by_value(img_in, -1, 1) # special case for inceptionv3\n",
    "  return img_in, loss\n",
    "  #END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK1SmII3KA7o"
   },
   "source": [
    "We are now ready to carry out the Deep Dream algorithm using gradient ascent, yipee ! Iterate 'n_iterations' times, each time adding an epsilon of the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fp7CApEuKA7o"
   },
   "outputs": [],
   "source": [
    "# first, reload image to make sure that we are not starting from a previous initialisation\n",
    "if (using_colab == True):\n",
    "\t#!wget 'https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg'\n",
    "\timg_in,_ = format_image('got.jpg',model_name)\n",
    "else:\n",
    "\timg_in,_ = format_image('images/got.jpg',model_name)\n",
    "\n",
    "img_in = tf.convert_to_tensor(img_in)\n",
    "if (model_name == 'inceptionv3'):\n",
    "\tstep = 0.1 # Gradient ascent step size\n",
    "elif (model_name == 'vgg16'):\n",
    "\tstep = 0.5 # Gradient ascent step size\n",
    "n_iterations = 40  # Number of gradient ascent steps\n",
    "\n",
    "for ii in range(0,n_iterations):\n",
    "\timg_in, loss = ... # FILL IN STUDENT\n",
    "\t\n",
    "\tif (ii%5==0):\n",
    "\t\tprint(\".\", end='')\n",
    "\t\timg_out = unformat_image(np.copy(img_in),model_name)\n",
    "\t\tplt.figure(num=None, figsize=(10, 8))\n",
    "\t\tplt.imshow(img_out)\n",
    "\t\tplt.show()\n",
    "    #plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LPemFEHKA7s"
   },
   "source": [
    "You can try different layers and switch between the two models and see what the results are !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52DV8fzHKA7s"
   },
   "source": [
    "## 3. Adversarial examples\n",
    "\n",
    "In this part of the lab work, we will explore the interesting case of adversarial examples. Adversarial examples are images which have been perturbed in a manner which __makes the network misclassify the image__.\n",
    "\n",
    "There are many ways to do this, however we can use a similar approach to the one used above, that is to say, we will use a gradient maximisation approach. This consists in iteratively adding the gradient of the loss with respect to the image, to the current image, in order to get a misclassified image.\n",
    "\n",
    "For this application, we need access to the last, classification, layer of the network. To do this, we set the ```include_top``` variable to ```True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUiO8tXDKA7t"
   },
   "outputs": [],
   "source": [
    "model_name = 'vgg16'#'inceptionv3'#\n",
    "# load model, including last (classification layer)\n",
    "if model_name == 'inceptionv3':\n",
    "\tmodel = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet')\n",
    "elif model_name == 'vgg16':\n",
    "\tmodel = tf.keras.applications.VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "# display architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mja7u8CMKA7x"
   },
   "source": [
    "You may have noticed an increase of the number of parameters in the network. This is normal, since we have included the last layer, the classification layer. This is a dense layer(s) and therefore has many parameters.\n",
    "\n",
    "\n",
    "We are nw going to take an image of a cat and try to misclassify it. First, read and format the image.\n",
    "\n",
    "NOTE !!!\n",
    "\n",
    "In the previous code, we were not interested in the dense layers, so the input image could be of __any size__ (there were only convolutional and maxpool layers). Now, since we are using the fully connected layers, we need to make sure the input is of the correct size for the images used in each network. For this, we redefine the format_image function to force the image to a certain size, depending on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_NztcSSKA7x"
   },
   "outputs": [],
   "source": [
    "def format_image_classif(img_file,model_name):\n",
    "    \"\"\"\n",
    "    This function reads and formats an image so that it can be fed to the VGG16 or InceptionV3 network.\n",
    "    In this case, we wish to force the image size to a certain shape, since we want to use the image for\n",
    "    classification\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_file : image file name\n",
    "    img_width : the target image width\n",
    "    img_height : he target image height\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img_out_model : the correctly formatted image for VGG16 or InceptionV3\n",
    "    img : the image as read by the load_img function of keras.preprocessing.image\n",
    "    \"\"\"\n",
    "    if model_name == 'inceptionv3':\n",
    "      img_width = 299\n",
    "      img_height = 299\n",
    "    elif model_name == 'vgg16':\n",
    "      img_width = 224\n",
    "      img_height = 224\n",
    "    # read image. Force the image size to a certain shape (uses a resize of the pillow package)\n",
    "    img = load_img(img_file,target_size=(img_height,img_width))\n",
    "    # convert image to an array\n",
    "    img_out = img_to_array(img)\n",
    "\n",
    "    if model_name == 'inceptionv3':\n",
    "      #https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input\n",
    "      #The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "      img_out_model = tf.keras.applications.inception_v3.preprocess_input(img_out)\n",
    "    elif model_name == 'vgg16':\n",
    "      #https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input\n",
    "      #The images are converted from RGB to BGR,\n",
    "      #then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "      img_out_model = tf.keras.applications.vgg16.preprocess_input(img_out)\n",
    "    # add a dimension at the beginning, coresponding to the batch dimension\n",
    "    img_out_model = np.expand_dims(img_out_model, axis=0)\n",
    "    return img_out_model, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkGPxU8JKA70"
   },
   "outputs": [],
   "source": [
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\"\n",
    "    img_in,img_visu = format_image_classif('cat_small.png',model_name)\n",
    "else:\n",
    "    img_in,img_visu = format_image_classif('images/cat_small.png',model_name)\n",
    "\n",
    "plt.imshow(img_visu)\n",
    "print(img_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UzQw4h-KA77"
   },
   "source": [
    "Now, we need to retrieve the last layer of the VGG or inceptionv3 networks. Do this in the same manner as above, but changing the name of the layer. To find the right name, look at the architecutre displayed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuNHDdAqKA77"
   },
   "outputs": [],
   "source": [
    "if(model_name == 'inceptionv3'): \n",
    "\tlayer_names = ...\t# FILL IN STUDENT\n",
    "elif (model_name == 'vgg16'):\n",
    "\tlayer_names = ...\t# FILL IN STUDENT\n",
    "layers = [model.get_layer(name).output for name in layer_names]\n",
    "# create a model which outputs the layer which interests us\n",
    "img_backend = model.input\n",
    "adversarial_model = tf.keras.Model(inputs=img_backend, outputs=layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRayrKxmKA7-"
   },
   "source": [
    "We are going to try to force the image to recognise a 'reflex_camera'. This is number 759 of the imagenet classes. You can use any one you like in fact (apart from ones linked to cats, obviously). To see the list of classes go to :\n",
    "\n",
    "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "\n",
    "Now, find the initial class of the cat image. Also display the top 5 classification results. For this, you can use the following functions :\n",
    "\n",
    "- model.predict(img) : classification prediction of the img variable\n",
    "- ```tf.keras.applications.vgg16.decode_predictions()``` or ```tf.keras.applications.inception_v3.decode_predictions()``` : converts the numerical probabilities in the y variable to human readable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D23TXNTtKA7_"
   },
   "outputs": [],
   "source": [
    "target_class = 759\n",
    "# carry out the network predictions on the example image\n",
    "y_predicted = ... # FILL IN STUDENT\n",
    "#define the true class as the initial most likely class\n",
    "true_class = np.zeros((1,y_predicted.shape[1]))\n",
    "true_class_ind = ... # FILL IN STUDENT\n",
    "true_class[0,true_class_ind] = 1 \n",
    "# print the \"true\" class, as well as the top 5 predicted classes, with the prediction probability\n",
    "if model_name == 'inceptionv3':\n",
    "  print(\"True class : \",...)\n",
    "  y_predicted_decoded =... # FILL IN STUDENT\n",
    "elif model_name == 'vgg16':\n",
    "  print(\"True class : \",...)\n",
    "  y_predicted_decoded = ... # FILL IN STUDENT\n",
    "print(y_predicted_decoded)\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkP6U09oKA8D"
   },
   "source": [
    "We must redefine the loss function for the adversarial model. Careful, the first axis of the activation is the batch dimension, the second one corresponds to the different categories.\n",
    "\n",
    "You must also redefine the gradient step function, so that it calls the new adversarial loss, and so that it takes a new parameter ```target_class``` as an input (other than this, the gradient step function is not modified, so you can look at your previous code as an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eawea0e4QWCx"
   },
   "outputs": [],
   "source": [
    "def adversarial_loss(model_in,img_in,target_class):\n",
    "\tactivation = model_in(img_in) \n",
    "\n",
    "\ttotal_loss = ...    # FILL IN STUDENT\n",
    "\n",
    "\treturn total_loss\n",
    "\n",
    "def adversarial_gradient_step(model_in, img_in, step_size, model_name,target_class):\n",
    "  # BEGIN STUDENT CODE\n",
    "  \n",
    "  img_in = ...  # FILL IN STUDENT\n",
    "\n",
    "  # END STUDENT CODE\n",
    "  if (model_name == 'inceptionv3'):\n",
    "    img_in = tf.clip_by_value(img_in, -1, 1) # special case for inceptionv3\n",
    "  return img_in, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZj67FUeR-_a"
   },
   "source": [
    "We are now ready to perturb the image such that we misclassify it. Youhoo !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y89wcKb1KA8K",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reload image, to make sure we are not starting from previous point\n",
    "if (using_colab == True):\n",
    "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\"\n",
    "    img_in,img_visu = format_image_classif('cat_small.png',model_name)\n",
    "else:\n",
    "    img_in,img_visu = format_image_classif('images/cat_small.png',model_name)\n",
    "\n",
    "#img_in = 10*np.random.randn(1,img_in.shape[1],img_in.shape[2],img_in.shape[3])\n",
    "#parameters\n",
    "img_in = tf.convert_to_tensor(img_in)\n",
    "if (model_name == 'inceptionv3'):\n",
    "  step = 0.01  # Gradient ascent step size\n",
    "elif(model_name == 'vgg16'):\n",
    "  step = 0.1  # Gradient ascent step size\n",
    "n_iterations = 100  # Number of gradient ascent steps\n",
    "for ii in range(0,n_iterations):\n",
    "    img_in, loss = ... # FILL IN STUDENT\n",
    "    if (ii%5==0):\n",
    "        img_show = unformat_image(np.copy(img_in),model_name)\n",
    "        plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_show)\n",
    "        #predict current model to see evolution of top classification\n",
    "        y_predicted = model.predict(img_in)\n",
    "        if (model_name == 'inceptionv3'):\n",
    "          print(tf.keras.applications.inception_v3.decode_predictions(y_predicted, top=1)[0])\n",
    "        if (model_name == 'vgg16'):\n",
    "          print(tf.keras.applications.vgg16.decode_predictions(y_predicted, top=1)[0])\n",
    "print('End of optimisation')\n",
    "# finally, display misclassified image\n",
    "img_show = unformat_image(np.copy(img_in),model_name)\n",
    "plt.figure(num=None, figsize=(10, 8))\n",
    "plt.imshow(img_show)\n",
    "#show the final top 5 classes\n",
    "y_predicted = model.predict(img_in)\n",
    "if (model_name == 'inceptionv3'):\n",
    "  print(tf.keras.applications.inception_v3.decode_predictions(y_predicted, top=5)[0])\n",
    "if (model_name == 'vgg16'):\n",
    "  print(tf.keras.applications.vgg16.decode_predictions(y_predicted, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uObcnTqBKA8N"
   },
   "source": [
    "As you should probably see, the image is changed such that it is no longer is correctly classified. It should be classified as a 'reflex camera', or whatever you chose, with high probability. This is a problem, since a human is still able to see a cat ! Furthermore, the top 5 classifications have nothing to do with cats !! Even bigger problem !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK0EcIzUKA8O"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for :\n",
    "- 1) Deep dream, ```retrieve_weights``` function : 2 points\n",
    "- 2) Deep dream, ```deep_dream_loss``` function : 1 point\n",
    "- 3) Deep dream, ```gradient_step``` function : 2 points\n",
    "- 4) Adversarial examples, choosing the right layer for the loss : 1 point\n",
    "- 5) Adversarial examples, ```y_predicted```, ```true_class``` and ```y_predicted_decoded``` : 3 Points\n",
    "- 6) Adversarial examples,  ```adversarial_loss``` function : 1 point\n",
    "\n",
    "Total over 10 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqCmIAL5sc1V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tp_deep_learning_cnn_part_2_for_students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
