{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "tp_deep_learning_cnn_part_1_for_students.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PKVAcrTVdcb"
      },
      "source": [
        "# TP Coding Convolutional Neural Networks in tensorflow/keras\n",
        "\n",
        "Author : Alasdair Newson\n",
        "\n",
        "alasdair.newson@telecom-paris.fr\n",
        "\n",
        "## Objective:\n",
        "\n",
        "We want to implement a Convolutional Neural Network (CNN) to do image classification. For this we will use two well-known image datasets :\n",
        "\n",
        "###MNIST\n",
        "\n",
        "Mnist is a dataset of 60,000 28$\\times$28 greyscale images of handwritten digits.\n",
        "\n",
        "### CIFAR 10\n",
        "\n",
        "CIFAR-10 dataset https://www.cs.toronto.edu/~kriz/cifar.html.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "### Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMQxnyAVdcb"
      },
      "source": [
        "# Load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r-L5nzGVdcc"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "print(tf.keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPkKAA3NVdcf"
      },
      "source": [
        "## Import data\n",
        "\n",
        "We are going to start with the mnist dataset. The mnist dataset has 10 classes which go from \"0\" to \"9\" (all the digits). We are going to truncate the number of elements in the database for faster training. Note that \"scalar\" means that the class is encoded as a scalar, rather than as a one-hot vector (we do the conversion afterwards)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdsR8TgupAmR"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "(X_train, Y_train_scalar), (X_test, Y_test_scalar) = mnist.load_data()\n",
        "\n",
        "n_max = 5000\n",
        "X_train = X_train[0:n_max,:,:]\n",
        "X_test = X_test[0:n_max,:,:]\n",
        "Y_train_scalar = Y_train_scalar[0:n_max]\n",
        "Y_test_scalar = Y_test_scalar[0:n_max]\n",
        "\n",
        "mnist_label_list = [ '0', '1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(np.unique(Y_train_scalar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7pZ6hCQp41Y"
      },
      "source": [
        "Now, let's look at some of the data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fe0vyM0pYe7"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for idx in range(0,10):\n",
        "    plt.subplot(2, 5, idx+1)\n",
        "    rand_ind = np.random.randint(0,X_train.shape[0])\n",
        "    plt.imshow(X_test[rand_ind,:,:],cmap='gray')\n",
        "    plt.title(mnist_label_list[int(Y_test_scalar[rand_ind])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRIydd6-qOVR"
      },
      "source": [
        "Now, we carry out some data pre-processing. In particular, we normalise the data to the range $[0,1]$ and convert the labels to a one-hot encoding :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "112VYdfgrHDl"
      },
      "source": [
        "# input image dimensions (mnist data is grey-level)\n",
        "img_rows, img_cols, nb_channels = X_train.shape[1], X_train.shape[2], 1\n",
        "\n",
        "# In case depth is 1 (black and white pictures) -> reshape to proper format\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, nb_channels)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, nb_channels)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "Y_train = to_categorical(Y_train_scalar)\n",
        "Y_test = to_categorical(Y_test_scalar)\n",
        "\n",
        "# number of classes\n",
        "nb_classes = Y_train.shape[1]\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "#print('y_train shape:', y_train.shape)\n",
        "print('Y_train shape:', Y_train.shape)\n",
        "print('There are {} train data'.format(X_train.shape[0]))\n",
        "print('There are {} test data'.format(X_test.shape[0]))\n",
        "print(img_rows, img_cols, nb_channels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slZYGH1ysFwj"
      },
      "source": [
        "## Defining the model for MNIST\n",
        "\n",
        "We will now define the simple CNN described below, for use with MNIST. The input of the CNN is a set of (28,28,1) image tensors. We apply :\n",
        "\n",
        "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
        "    - a ReLu activation function\n",
        "    \n",
        "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
        "    - a ReLu activation function\n",
        "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
        "    \n",
        "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
        "    - A softmax activation function which outputs are the $P(y_c | X)$ (multi-class problem)\n",
        "\n",
        "We define the parameters of the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1MPFemUrDg4"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "# --- Size of the successive layers\n",
        "n_h_0 = nb_channels  # number of input channels\n",
        "n_h_1 = nb_filters\n",
        "n_h_2 = nb_filters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvMx580MsAFF"
      },
      "source": [
        "## Creating the CNN model using the Sequential API\n",
        "\n",
        "Tensorflow has a simple way of adding layers to create a neural network. First, you can indicate to Tensorflow that the model is 'sequential', that is to say, a straight-forward CNN. For this, you can use the following function :\n",
        "- ```model = Sequential()```\n",
        "\n",
        "After this, you can add layers with the function.\n",
        "\n",
        "- ```model.add()```\n",
        "\n",
        "You can then use the ```Conv2D```, ```Activation```, ```MaxPooling2D```, ```Flatten``` and ```Dense``` (fully connected) functions to specify different layer types. Note that in the case of this approach, you will have to specify the input image size in the first layer of the network. So, for example, if the first layer is convolutional :\n",
        "\n",
        "- model = Sequential()\n",
        "- model.add(Conv2D(nb_filters, kernel_size, input_shape=input_shape, name='Conv1'))\n",
        "\n",
        "\n",
        "## Creating the CNN model using the standard API\n",
        "\n",
        "Otherwise, another approach to creating the model is to explicitly create the input variable, and just cascade the different functions, as in Tensorflow. So, for the same example, we would have :\n",
        "\n",
        "- input = Input(shape=(img_rows,img_cols,nb_channels))\n",
        "- output = Conv2D(input_shape=self.img_shape,filters=nb_filters,kernel_size=kernel_size)(input)\n",
        "- model = Model(input, output)\n",
        "\n",
        "Create your CNN now with the network parameters specified above :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzWrY8nrsx62"
      },
      "source": [
        "# --- START CODE HERE\n",
        "\n",
        "model = ...\n",
        "\n",
        "# --- END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Z5rywAtaWx"
      },
      "source": [
        "Now, you need to :\n",
        "\n",
        "- compile\n",
        "- display (```summary```)\n",
        "- train the model\n",
        "\n",
        "and show the test accuracy once the training is finished. You should use the ```optimizers.Adam``` optimiser, and the correct loss (which one ?)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q61ACIG2thbI"
      },
      "source": [
        "# compile, train and evaluate model\n",
        "# BEGIN STUDENT CODE\n",
        "\n",
        "# compile model\n",
        "\n",
        "# display model\n",
        "\n",
        "# fit model\n",
        "\n",
        "# END STUDENT CODE\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=False)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_8TwXAAvAE6"
      },
      "source": [
        "You should achieve a test accuracy of around 0.94 within 10 epochs.\n",
        "\n",
        "This is pretty good ! However, don't get too excited just yet. Unfortunately, one of the golden rules of deep learning is that everything works with mnist (unless you have gotten the model really wrong). So often, mnist is just used as a sanity check. Let's now look at a more complex dataset : CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMXvoBa2xsZe"
      },
      "source": [
        "## 2/ CNN on CIFAR10\n",
        "\n",
        "Cifar10 has ten categories, which are much more complex than the digits in mnist. In this case, we are going to add more layers to increase the capacity of the network. We create the following network :\n",
        "\n",
        "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
        "    - a ReLu activation function\n",
        "    \n",
        "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
        "    - a ReLu activation function\n",
        "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
        "    \n",
        "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
        "    - a ReLu activation function\n",
        "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
        "    \n",
        "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
        "    - A softmax activation function which outputs are the $P(y_c | X)$ (multi-class problem)\n",
        "\n",
        "\n",
        "As previsously, we investigate the data, display some samples, and we carry out some pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fnyiIdsoYzS"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCeqiu0Vdcg"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, Y_train_scalar), (X_test, Y_test_scalar) = cifar10.load_data()\n",
        "\n",
        "print(\"Train data shape : \", X_train.shape)\n",
        "print(\"Test data shape : \", X_test.shape)\n",
        "print(\"Number of data categories : \", np.unique(Y_train_scalar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmE6agVqVdcj"
      },
      "source": [
        "The CIFAR-10 dataset has 10 classes. These are the following :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeuLaea5Vdcj"
      },
      "source": [
        "cifar_10_list = [ 'airplane', 'automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF_wRtfNVdcm"
      },
      "source": [
        "### Display some of the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6nQcjAVdcm"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for idx,i in enumerate(range(100,110)):\n",
        "    plt.subplot(2, 5, idx+1)\n",
        "    plt.imshow(X_train[i, :, :, :])\n",
        "    plt.title(cifar_10_list[int(Y_train_scalar[i])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPtW0s-pVdcp"
      },
      "source": [
        "As you can see, the images are in quite small resolution. This makes them more easy to handle computationally, however it also means that they are quite difficult to analyse, even for a human being. Therefore, __you might not get that great accuracy scores__. The goal is not to get great scores (this would take longer training and potentially a larger network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvNQiBcsVdcp"
      },
      "source": [
        "## Format conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLf4-_eQVdcq"
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols, nb_channels = 32, 32, 3\n",
        "nb_classes = 10\n",
        "\n",
        "# In case depth is 1 (black and white pictures) -> reshape to proper format\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, nb_channels)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, nb_channels)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# convert to one-hot encoding\n",
        "Y_train = to_categorical(Y_train_scalar)\n",
        "Y_test = to_categorical(Y_test_scalar)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('Y_train_scalar shape:', Y_train_scalar.shape)\n",
        "print('Y_train shape (one hot encoding):', Y_train.shape)\n",
        "print('There is {} train data'.format(X_train.shape[0]))\n",
        "print('There is {} test data'.format(X_test.shape[0]))\n",
        "print(img_rows, img_cols, nb_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJggytbVdcs"
      },
      "source": [
        "For computational reasons, we are going to reduce the amount of training data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml557ypyVdcs"
      },
      "source": [
        "n_train_samples = 10000\n",
        "X_train = X_train[0:n_train_samples,:,:,:]\n",
        "Y_train = Y_train[0:n_train_samples,:]\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhYIugXfVdcv"
      },
      "source": [
        "\n",
        "\n",
        "We are now going to create the model described just above, and train it on the previously loaded CIFAR dataset.\n",
        "\n",
        "## We define the parameters of the model, and of the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CR4DnDfVdcw"
      },
      "source": [
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 20\n",
        "batch_size = 64\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32 \n",
        "\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "\n",
        "# --- Size of the successice layers\n",
        "n_h_0 = nb_channels\n",
        "n_h_1 = nb_filters\n",
        "n_h_2 = nb_filters\n",
        "n_h_3 = nb_filters\n",
        "\n",
        "input_shape = (img_rows, img_cols, nb_channels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaIkcutSVdcy"
      },
      "source": [
        "Now, define the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVsNB3l-Vdcy"
      },
      "source": [
        "# --- START CODE HERE\n",
        "model = ...\n",
        "\n",
        "# --- END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq5ZRK_aVdc0"
      },
      "source": [
        "Again, compile, display and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdIoVk0Vdc1"
      },
      "source": [
        "# START CODE HERE\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26RA5S8nVdc-"
      },
      "source": [
        "Evaluate the performances of the model on the test data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K40PBXAVdc_"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=False)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLFIFGUGW7MB"
      },
      "source": [
        "The train accuracy after 20 epochs should be around 0.4-0.5 (on the test data).\n",
        "\n",
        "I guess you may be thinking \"0.5, that's not very good, what's the deal ? I am pretty disappointed\". Some of you may have even found accuracies much lower, depending on your luck with the initialisation.\n",
        "\n",
        "Well 0.5 is not great, its true, but it's not that bad considering there are 10 categories. A random choice would give an accuracy of 0.1. However, there are several points to consider here :\n",
        "\n",
        "- The data is more complex than that of mnist for example : there are a lot more variations of \"dog\"s than \"1\"s. Therefore, the network has to be larger\n",
        "- The resolution is  32$\\times$32, therefore even for humans it is difficult to recognise some of the images\n",
        "- Unfortunately, when we start dealing with larger networks, the possibilities for falling into local minima become more and more present. There is no way around this more the moment, due to lack of theoretical understanding. If you do have an idea of how to do this, please email me and we will most likely become very rich.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrDkWLFhVddB"
      },
      "source": [
        "### Inspecting the network's results\n",
        "\n",
        "Let's take a look at what the network has learned. What do you think ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD3n19mPVddB"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for idx,i in enumerate(range(100,110)):\n",
        "    plt.subplot(2, 5, idx+1)\n",
        "    rand_ind = np.random.randint(0,X_test.shape[0])\n",
        "    predicted_class = ... # FILL IN CODE\n",
        "    plt.imshow(X_test[rand_ind,:,:,:])\n",
        "    plt.title(cifar_10_list[int(predicted_class)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTy-bx9dVddD"
      },
      "source": [
        "## Exploring the model\n",
        "\n",
        "You might want to look around inside the network to see what convolutional filters have been learned.\n",
        "\n",
        "You can explore the network parameters (relatively) easily with Tensorflow. For example ```model.layers``` is a list of the layers of the network. Each element of the network contains the information necessary for this layer. To show the content of a layer i, you can type :\n",
        "\n",
        "```dir(model.layers[i])```\n",
        "\n",
        "Let us try to inspect the weights (parameters) of the convolutional network. You can do this in a similar fashion to the way you did this in the RNN lab work (when you displayed the embedding matrix), using :\n",
        "\n",
        "- layers\n",
        "- get_weights()\n",
        "\n",
        " Note that the get_weights() function returns the following :\n",
        "- [weights, biases] if the layer is convolutional. weights has size [m,n,filter_depth,n_filters]\n",
        "- an empty list otherwise\n",
        "\n",
        "Now, display below all the filter weights of the first layer and for the first channel as images (plt.imshow). Careful, the first layer can be either the ```Input``` or a convolutional layer, depending on which API you used :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STEbiT8TVddE"
      },
      "source": [
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for num in range(0,32):\n",
        "    plt.subplot(8, 4, num+1)\n",
        "    plt.imshow(...) # FILL IN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9t1RMsyaZfy"
      },
      "source": [
        "What do you think ? Pretty incomprehensible no ? Do not spend too long trying to interpret these filters, that way madness lies. Indeed, they say that you can find some researchers locked in their offices staring at convolutional filters, trying to find some semblance of meaning ...\n",
        "\n",
        "So, it seems that understanding a CNN by just looking at the filters is an exercise in futility. How can we do better ? Well, take a look at part 2 of the lab !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btp5WgPWVddF"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for \n",
        "- 1) MNIST : Defining the first model : 3 points\n",
        "- 2) MNIST : Compiling and training the first model : 1 point\n",
        "- 3) CIFAR10 : Defining the second model : 3 points\n",
        "- 4) CIFAR10 : Compiling and training the first model : 1 point\n",
        "- 5) Calculating ```predicted_class``` : 1 point\n",
        "- 6) Correctly visualising the weights : 1 point\n",
        "\n",
        "\n",
        "Total over 10 points. For the questions with three points :\n",
        "- 1 point if partially correct\n",
        "- 2 points if correct code but does not give the expected results or if the code does not compile. Be careful, sometimes the training might get stuck quickly by bad luck : restart the code a few times to make sure it is indeed a bug.\n",
        "- 3 points for correct code and correct execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXq3GF_WPubU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}